from celery import shared_task
from celery.canvas import signature, Signature
from dacite.core import from_dict
from dacite.config import Config

from kraken.core.types import (
    Crawl,
    Stage,
)
from kraken.core.tasks import DatabaseTask
from kraken.utils import mongodb_key_sanitizer as sanitizer


# TODO: Add option to disable metrics, weights and statistics logging
# TODO: Expand and rewrite documentation.
@shared_task(
    bind=True,
    base=DatabaseTask,
    name="kraken.callback.crawl_monitor",
)
def crawl_monitor_callback(
    self, stage: Stage, crawl_id: str, final_stage: bool
) -> None:
    """Adds the progress of a stage to the corresponding crawl. PipelineResults
    fields which are prefixed with '__' are ignored.

    Args:
        stage (Stage): Stage for which the progress should be added to the crawl.
        crawl_id (str): ID of the crawl to which the stage belongs.

    Returns:
        None
    """
    # If stage is serialized try to deserialize
    if not isinstance(stage, Stage):
        stage = from_dict(
            data_class=Stage,
            data=stage,
            config=Config(type_hooks={Signature: signature}),
        )

    # Dictionary in which we collect all the necessary query's to update the crawl
    update = {}

    # Add query to increment the finished stage
    update[f"inc__progress__{sanitizer(stage.name)}__finished"] = 1

    # Add queries to monitor cost and gain of the stage
    update[f"inc__progress__{sanitizer(stage.name)}__cost"] = stage.progress.cost
    update[f"inc__progress__{sanitizer(stage.name)}__gain"] = stage.progress.gain

    # Add queries to monitor the triggered terminators
    for terminator_name, triggered in stage.progress.terminated_by.items():
        update[
            f"inc__progress__{sanitizer(stage.name)}__terminated_by__{sanitizer(terminator_name)}"
        ] = (1 if triggered else 0)

    # Add queries to monitor the statistics, metrics and weight of each pipeline
    for pipeline_name, pipeline_result in stage.progress.pipeline_results.items():
        # Handle the statistics for each pipeline. Statistics get aggregated on
        # the pipeline level.
        for key, value in pipeline_result.statistics.items():
            # Filter all keys prefixed with '__'
            if key.startswith("__"):
                pass
            # Add query for each remaining key
            else:
                update[
                    f"inc__progress__{sanitizer(stage.name)}__pipelines__{sanitizer(pipeline_name)}__{sanitizer(key)}"
                ] = value

        # Handle the metrics generated by each pipeline. Metrics get aggregated
        # on the stage level. TODO: Add check for duplicate metric keys.
        for key, value in pipeline_result.metrics.items():
            # Filter all keys prefixed with '__'
            if key.startswith("__"):
                pass
            # Add query for each remaining key
            else:
                update[
                    f"inc__progress__{sanitizer(stage.name)}__metrics__{sanitizer(key)}"
                ] = value

        # Handle the weight for each pipeline. Metrics get aggregated on the
        # stage level. TODO: Add check for duplicate weights.
        if pipeline_result.weight is not None:
            update[
                f"inc__progress__{sanitizer(stage.name)}__weight"
            ] = pipeline_result.weight

    # Dispatch all collected updates
    Crawl.objects(id=crawl_id).update(**update)  # type: ignore
